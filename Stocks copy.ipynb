{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pytorch_lightning' has no attribute 'utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mace_tools_open\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mneuralprophet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NeuralProphet\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mneuralprophet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfigSeasonality\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\neuralprophet\\__init__.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load, save, set_log_level, set_random_seed  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Reduce lightning logs\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutilities\u001b[49m\u001b[38;5;241m.\u001b[39mwarnings\u001b[38;5;241m.\u001b[39mPossibleUserWarning)\n\u001b[0;32m     16\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_lightning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mWARNING)\n\u001b[0;32m     18\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pytorch_lightning' has no attribute 'utilities'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import ace_tools_open as tools\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import ace_tools_open as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neuralprophet in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: captum>=0.6.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (0.8.0)\n",
      "Requirement already satisfied: holidays>=0.41 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (0.72)\n",
      "Requirement already satisfied: kaleido==0.2.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (0.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.5.3 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (3.9.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.25.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (2.2.3)\n",
      "Requirement already satisfied: plotly>=5.13.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (6.1.0)\n",
      "Requirement already satisfied: pytorch-lightning>=2.0.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (2.1.3)\n",
      "Requirement already satisfied: tensorboard>=2.11.2 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (2.18.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (2.1.2)\n",
      "Requirement already satisfied: torchmetrics>=1.0.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (1.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from neuralprophet) (4.12.2)\n",
      "Requirement already satisfied: packaging in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from captum>=0.6.0->neuralprophet) (24.2)\n",
      "Requirement already satisfied: tqdm in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from captum>=0.6.0->neuralprophet) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from holidays>=0.41->neuralprophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from matplotlib>=3.5.3->neuralprophet) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from matplotlib>=3.5.3->neuralprophet) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from matplotlib>=3.5.3->neuralprophet) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from matplotlib>=3.5.3->neuralprophet) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from matplotlib>=3.5.3->neuralprophet) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from matplotlib>=3.5.3->neuralprophet) (3.2.3)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from matplotlib>=3.5.3->neuralprophet) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.5.3->neuralprophet) (3.21.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from pandas>=2.0.0->neuralprophet) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from pandas>=2.0.0->neuralprophet) (2025.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from plotly>=5.13.1->neuralprophet) (1.39.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from python-dateutil->holidays>=0.41->neuralprophet) (1.17.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from pytorch-lightning>=2.0.0->neuralprophet) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (2025.3.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from pytorch-lightning>=2.0.0->neuralprophet) (0.14.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralprophet) (3.10)\n",
      "Requirement already satisfied: setuptools in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from lightning-utilities>=0.8.0->pytorch-lightning>=2.0.0->neuralprophet) (75.8.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from tensorboard>=2.11.2->neuralprophet) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from tensorboard>=2.11.2->neuralprophet) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from tensorboard>=2.11.2->neuralprophet) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from tensorboard>=2.11.2->neuralprophet) (5.29.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from tensorboard>=2.11.2->neuralprophet) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from tensorboard>=2.11.2->neuralprophet) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.11.2->neuralprophet) (8.6.1)\n",
      "Requirement already satisfied: filelock in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from torch>=2.0.0->neuralprophet) (3.18.0)\n",
      "Requirement already satisfied: sympy in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from torch>=2.0.0->neuralprophet) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from torch>=2.0.0->neuralprophet) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from torch>=2.0.0->neuralprophet) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from tqdm->captum>=0.6.0->neuralprophet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.11.2->neuralprophet) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\vscode\\research methodology\\.venv\\lib\\site-packages (from sympy->torch>=2.0.0->neuralprophet) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install neuralprophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for all uploaded stock data\n",
    "file_paths = {\n",
    "    # Consumer Goods Sector\n",
    "    \"CPIN\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Consumer Goods/CPIN.JK_dataprice.xlsx\",\n",
    "    \"ICBP\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Consumer Goods/ICBP.JK_dataprice.xlsx\",\n",
    "    \"INDF\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Consumer Goods/INDF.JK_dataprice.xlsx\",\n",
    "    \"UNVR\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Consumer Goods/UNVR.JK_dataprice.xlsx\",\n",
    "    \n",
    "    # Energy and Utilities Sector\n",
    "    \"ADRO\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Energy and Utils/ADRO.JK_dataprice.xlsx\",\n",
    "    \"PGAS\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Energy and Utils/PGAS.JK_dataprice.xlsx\",\n",
    "    \"PTBA\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Energy and Utils/PTBA.JK_dataprice.xlsx\",\n",
    "\n",
    "    # Finance Sector\n",
    "    \"BBCA\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Finance/BBCA.JK_dataprice.xlsx\",\n",
    "    \"BBNI\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Finance/BBNI.JK_dataprice.xlsx\",\n",
    "    \"BBTN\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Finance/BBTN.JK_dataprice.xlsx\",\n",
    "    \"BMRI\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Finance/BBTN.JK_dataprice.xlsx\",\n",
    "    \"BBRI\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Finance/BBRI.JK_dataprice.xlsx\",\n",
    "    \n",
    "    # Healthcare Sector\n",
    "    \"KLBF\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Healthcare/KLBF.JK_dataprice.xlsx\",\n",
    "    \"MIKA\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Healthcare/MIKA.JK_dataprice.xlsx\",\n",
    "    \n",
    "    \n",
    "    # Mines and Basic Materials Sector\n",
    "    \"ANTM\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Mines and Basic Materials/ANTM.JK_dataprice.xlsx\",\n",
    "    \"BRPT\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Mines and Basic Materials/BRPT.JK_dataprice.xlsx\",\n",
    "    \"INCO\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Mines and Basic Materials/INCO.JK_dataprice.xlsx\",\n",
    "    \"INKP\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Mines and Basic Materials/INKP.JK_dataprice.xlsx\",\n",
    "    \"MDKA\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Mines and Basic Materials/MDKA.JK_dataprice.xlsx\",\n",
    "    \"SMGR\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Mines and Basic Materials/SMGR.JK_dataprice.xlsx\",\n",
    "    \"TINS\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Mines and Basic Materials/TINS.JK_dataprice.xlsx\",\n",
    "    \"UNTR\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Mines and Basic Materials/UNTR.JK_dataprice.xlsx\",\n",
    "\n",
    "    # Otomotive and Industrial Sector\n",
    "    \"ASII\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Otomotive and Industrials/ASII.JK_dataprice.xlsx\",\n",
    "\n",
    "    # Tech Sector\n",
    "    \"BUKA\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Tech/BUKA.JK_dataprice.xlsx\",\n",
    "    \"EMTK\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Tech/EMTK.JK_dataprice.xlsx\",\n",
    "    \"GOTO\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Tech/GOTO.JK_dataprice.xlsx\",\n",
    "\n",
    "    # Telecommunication Sector\n",
    "    \"EXCL\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Telecommunications/EXCL.JK_dataprice.xlsx\",\n",
    "    \"TBIG\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Telecommunications/TBIG.JK_dataprice.xlsx\",\n",
    "    \"TLKM\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Telecommunications/TLKM.JK_dataprice.xlsx\",\n",
    "    \"TOWR\": \"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Telecommunications/TOWR.JK_dataprice.xlsx\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the fixed NeuralProphet code with all recent updates\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# === Clean NeuralProphet Forecast with Plotly ===\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mneuralprophet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NeuralProphet\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load and prepare the data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\neuralprophet\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# make core features and version number accessible\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mpropagate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\__init__.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprediction_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BasePredictionWriter\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprogress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProgressBar, RichProgressBar, TQDMProgressBar\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpruning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelPruning\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrich_model_summary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RichModelSummary\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_weight_avg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StochasticWeightAveraging\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\pruning.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightningModule\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MisconfigurationException\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrank_zero\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rank_zero_debug, rank_zero_only\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\core\\__init__.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightningModule\n\u001b[0;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLightningDataModule\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\core\\module.py:63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _load_from_checkpoint\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Logger\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m call\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger_connector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx_validator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _FxValidator\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradClipAlgorithmType\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\__init__.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed_everything\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorBoardLogger\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log_hyperparams\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _PredictionLoop, _TrainingEpochLoop\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_loop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _EvaluationLoop\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfit_loop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _FitLoop\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\loops\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _Loop  \u001b[38;5;66;03m# noqa: F401 isort: skip (avoids circular imports)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_loop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _EvaluationLoop  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfit_loop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _FitLoop  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _AutomaticOptimization, _ManualOptimization  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _Loop\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprogress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _BatchProgress\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _no_grad_context, _select_data_fetcher, _verify_dataloader_idx_requirement\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m call\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_connector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     _check_dataloader_iterable,\n\u001b[0;32m     33\u001b[0m     _DataLoaderSource,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     _resolve_overfit_batches,\n\u001b[0;32m     38\u001b[0m )\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfetchers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _DataFetcher, _DataLoaderIterDataFetcher, _PrefetchDataFetcher\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprogress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _BaseProgress\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FSDPStrategy\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelStrategy\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Strategy\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\strategies\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _StrategyRegistry\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDPStrategy  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepSpeedStrategy  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FSDPStrategy  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\strategies\\ddp.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LightningModuleWrapperBase, _LightningPrecisionModuleWrapperBase\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _register_ddp_comm_hook, _sync_module_states, prepare_for_backward\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecisionPlugin\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunchers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MultiProcessingLauncher, _SubprocessScriptLauncher\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelStrategy\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\plugins\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhpu_plugin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HPUCheckpointIO\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_sync\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayerSync, TorchSyncBatchNorm\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MixedPrecisionPlugin\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepSpeedPrecisionPlugin\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdouble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DoublePrecisionPlugin\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepSpeedPrecisionPlugin\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdouble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DoublePrecisionPlugin\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FSDPMixedPrecisionPlugin\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhpu\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HPUPrecisionPlugin\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mipu\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IPUPrecisionPlugin\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\fsdp.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MisconfigurationException\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TORCH_GREATER_EQUAL_1_12 \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfully_sharded_data_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MixedPrecision\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharded_grad_scaler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardedGradScaler\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\distributed\\fsdp\\__init__.py:11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_flat_param\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlatParameter \u001b[38;5;28;01mas\u001b[39;00m FlatParameter\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fully_shard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     CPUOffloadPolicy,\n\u001b[0;32m      4\u001b[0m     FSDPModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     UnshardHandle,\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfully_sharded_data_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     BackwardPrefetch,\n\u001b[0;32m     13\u001b[0m     CPUOffload,\n\u001b[0;32m     14\u001b[0m     FullOptimStateDictConfig,\n\u001b[0;32m     15\u001b[0m     FullStateDictConfig,\n\u001b[0;32m     16\u001b[0m     FullyShardedDataParallel,\n\u001b[0;32m     17\u001b[0m     LocalOptimStateDictConfig,\n\u001b[0;32m     18\u001b[0m     LocalStateDictConfig,\n\u001b[0;32m     19\u001b[0m     MixedPrecision,\n\u001b[0;32m     20\u001b[0m     OptimStateDictConfig,\n\u001b[0;32m     21\u001b[0m     OptimStateKeyType,\n\u001b[0;32m     22\u001b[0m     ShardedOptimStateDictConfig,\n\u001b[0;32m     23\u001b[0m     ShardedStateDictConfig,\n\u001b[0;32m     24\u001b[0m     ShardingStrategy,\n\u001b[0;32m     25\u001b[0m     StateDictConfig,\n\u001b[0;32m     26\u001b[0m     StateDictSettings,\n\u001b[0;32m     27\u001b[0m     StateDictType,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     31\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# FSDP1\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackwardPrefetch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnshardHandle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     57\u001b[0m ]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Set namespace for exposed private names\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\distributed\\fsdp\\fully_sharded_data_parallel.py:32\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_common_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     _FSDPState,\n\u001b[0;32m     25\u001b[0m     _get_param_to_fqns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     TrainingState,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _annotate_modules_for_dynamo\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_init_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     _check_orig_params_flattened,\n\u001b[0;32m     34\u001b[0m     _init_buffer_state,\n\u001b[0;32m     35\u001b[0m     _init_core_state,\n\u001b[0;32m     36\u001b[0m     _init_device_handle,\n\u001b[0;32m     37\u001b[0m     _init_extension,\n\u001b[0;32m     38\u001b[0m     _init_ignored_module_states,\n\u001b[0;32m     39\u001b[0m     _init_param_handle_from_module,\n\u001b[0;32m     40\u001b[0m     _init_prefetching_state,\n\u001b[0;32m     41\u001b[0m     _init_process_group_state,\n\u001b[0;32m     42\u001b[0m     _init_runtime_state,\n\u001b[0;32m     43\u001b[0m     _init_state_dict_state,\n\u001b[0;32m     44\u001b[0m     HYBRID_SHARDING_STRATEGIES,\n\u001b[0;32m     45\u001b[0m     ProcessGroupType,\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_runtime_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     _get_fsdp_root_states,\n\u001b[0;32m     49\u001b[0m     _is_fsdp_root,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     _wait_for_computation_stream,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_wrap_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _auto_wrap\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\distributed\\fsdp\\_init_utils.py:45\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     BackwardPrefetch,\n\u001b[0;32m     36\u001b[0m     CPUOffload,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     StateDictType,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _Policy\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTensorExtensions\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _sync_params_and_buffers\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\distributed\\tensor\\parallel\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parallelize_module\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loss_parallel\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstyle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     ColwiseParallel,\n\u001b[0;32m      6\u001b[0m     ParallelStyle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     SequenceParallel,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\distributed\\tensor\\parallel\\api.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_mesh\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _mesh_resources, DeviceMesh\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _validate_tp_mesh_dim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstyle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelStyle\n\u001b[0;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallelize_module\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\distributed\\tensor\\parallel\\_utils.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplacement_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Placement\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_compiling \u001b[38;5;28;01mas\u001b[39;00m is_torchdynamo_compiling\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_torchdynamo_compiling\u001b[39m():  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:57\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_method\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     58\u001b[0m     config,\n\u001b[0;32m     59\u001b[0m     exc,\n\u001b[0;32m     60\u001b[0m     graph_break_hints,\n\u001b[0;32m     61\u001b[0m     logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging,\n\u001b[0;32m     62\u001b[0m     trace_rules,\n\u001b[0;32m     63\u001b[0m     variables,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     66\u001b[0m     get_indexof,\n\u001b[0;32m     67\u001b[0m     JUMP_OPNAMES,\n\u001b[0;32m     68\u001b[0m     livevars_analysis,\n\u001b[0;32m     69\u001b[0m     propagate_line_nums,\n\u001b[0;32m     70\u001b[0m )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     72\u001b[0m     cleaned_instructions,\n\u001b[0;32m     73\u001b[0m     create_call_function,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     unique_id,\n\u001b[0;32m     81\u001b[0m )\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresume_execution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TORCH_DYNAMO_RESUME_IN_PREFIX\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getfile, hashable, NP_SUPPORTED_MODULES, unwrap_if_wrapper\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     BuiltinVariable,\n\u001b[0;32m     34\u001b[0m     FunctionalCallVariable,\n\u001b[0;32m     35\u001b[0m     FunctorchHigherOrderVariable,\n\u001b[0;32m     36\u001b[0m     LocalGeneratorFunctionVariable,\n\u001b[0;32m     37\u001b[0m     LocalGeneratorObjectVariable,\n\u001b[0;32m     38\u001b[0m     NestedUserFunctionVariable,\n\u001b[0;32m     39\u001b[0m     PolyfilledFunctionVariable,\n\u001b[0;32m     40\u001b[0m     SkipFunctionVariable,\n\u001b[0;32m     41\u001b[0m     TorchInGraphFunctionVariable,\n\u001b[0;32m     42\u001b[0m     UserFunctionVariable,\n\u001b[0;32m     43\u001b[0m     UserMethodVariable,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     47\u001b[0m np: Optional[types\u001b[38;5;241m.\u001b[39mModuleType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Vscode\\Research Methodology\\.venv\\lib\\site-packages\\torch\\_dynamo\\variables\\__init__.py:19\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package implements variable tracking and symbolic execution capabilities for Dynamo,\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mwhich are essential for converting Python code into FX graphs. It provides a comprehensive\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mallows Dynamo to accurately trace and optimize Python code while preserving its semantics.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VariableTracker\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuiltin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BuiltinVariable\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConstantVariable, EnumVariable\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:786\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:881\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:979\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Clean NeuralProphet Forecast with Plotly ===\n",
    "# Load and prepare the data\n",
    "df = pd.read_excel(\"CS-IDX30 version 3/CS-IDX30 version 3/pricehistory data/Finance/BBCA.JK_dataprice.xlsx\", sheet_name=\"Sheet1\")\n",
    "df = df[['Date', 'Close']].rename(columns={'Date': 'ds', 'Close': 'y'})\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "df = df.dropna()\n",
    "\n",
    "# Confirm data is clean\n",
    "assert df['y'].apply(lambda x: isinstance(x, (int, float))).all()\n",
    "\n",
    "# Create and train the model without checkpointing (fixes PyTorch 2.6 error)\n",
    "model = NeuralProphet(n_lags=5, n_forecasts=1, epochs=100)\n",
    "model.fit(df, freq='D', minimal=True, checkpointing=False, continue_training=False)\n",
    "\n",
    "# Forecast\n",
    "future = model.make_future_dataframe(df, periods=10, n_historic_predictions=True)\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['y'], mode='lines', name='Actual'))\n",
    "fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat1'], mode='lines', name='Forecast (t+1)'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='NeuralProphet Forecast vs Actual',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for CPIN:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  2530.030696  2575.699120  2365.624369  2438.693848  66502600   \n",
      "2017-08-01  2466.094728  2575.698938  2447.827360  2530.030518  29682500   \n",
      "2017-08-02  2530.030518  2575.698938  2520.896833  2530.030518  13380800   \n",
      "2017-08-03  2530.030518  2548.297886  2511.763149  2530.030518   6098000   \n",
      "2017-08-04  2484.362266  2593.966484  2484.362266  2566.565430   7424900   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31          0             0  \n",
      "2017-08-01          0             0  \n",
      "2017-08-02          0             0  \n",
      "2017-08-03          0             0  \n",
      "2017-08-04          0             0  \n",
      "Data for ICBP:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  7590.259146  7635.171922  7433.064430  7500.433594  11906600   \n",
      "2017-08-01  7455.520663  7500.433438  7410.607888  7477.977051   6721800   \n",
      "2017-08-02  7455.520996  7500.433773  7343.239053  7455.520996   4626600   \n",
      "2017-08-03  7410.608219  7477.977385  7388.151830  7455.520996   4140600   \n",
      "2017-08-04  7410.608219  7500.433773  7365.695442  7455.520996   5412100   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31          0             0  \n",
      "2017-08-01          0             0  \n",
      "2017-08-02          0             0  \n",
      "2017-08-03          0             0  \n",
      "2017-08-04          0             0  \n",
      "Data for INDF:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  7270.154268  7355.685494  7056.326201  7163.240234  12752100   \n",
      "2017-08-01  7184.623518  7227.389134  7034.943861  7099.092285   9058200   \n",
      "2017-08-02  7099.092285  7141.857901  7056.326669  7099.092285   4753300   \n",
      "2017-08-03  7099.091785  7141.857398  7034.943365  7056.326172   6708500   \n",
      "2017-08-04  7034.943359  7120.474586  6992.177746  7034.943359   6600200   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31          0             0  \n",
      "2017-08-01          0             0  \n",
      "2017-08-02          0             0  \n",
      "2017-08-03          0             0  \n",
      "2017-08-04          0             0  \n",
      "Data for UNVR:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  8437.002049  8691.339948  8366.839870  8586.096680  17823000   \n",
      "2017-08-01  8612.405726  8612.405726  8489.621938  8502.777344   6125000   \n",
      "2017-08-02  8502.777344  8511.547614  8463.311126  8502.777344   6305500   \n",
      "2017-08-03  8463.310864  8472.081135  8362.452755  8366.837891  13967500   \n",
      "2017-08-04  8353.683357  8375.609035  8336.142815  8344.913086  10639000   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31          0             0  \n",
      "2017-08-01          0             0  \n",
      "2017-08-02          0             0  \n",
      "2017-08-03          0             0  \n",
      "2017-08-04          0             0  \n",
      "Data for ADRO:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  1240.160481  1240.160481  1212.978882  1212.978882  21694000   \n",
      "2017-08-01  1240.160317  1287.728110  1240.160317  1263.944214  75698700   \n",
      "2017-08-02  1287.728149  1304.716647  1250.353454  1287.728149  49377700   \n",
      "2017-08-03  1287.728219  1287.728219  1250.353521  1253.751221  24517600   \n",
      "2017-08-04  1253.751367  1287.728369  1243.558267  1270.739868  28084500   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for PGAS:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  1816.897705  1824.972806  1800.747503  1816.897705  14035500   \n",
      "2017-08-01  1816.897905  1824.973007  1760.372192  1760.372192  32919300   \n",
      "2017-08-02  1744.221924  1792.672533  1736.146822  1744.221924  22791100   \n",
      "2017-08-03  1744.221896  1768.447200  1744.221896  1752.296997  13666200   \n",
      "2017-08-04  1752.296931  1776.522234  1736.146729  1736.146729  18739300   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for PTBA:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  1559.190790  1579.901104  1550.314941  1550.314941  15087500   \n",
      "2017-08-01  1585.818235  1612.445780  1579.901003  1582.859619  21720000   \n",
      "2017-08-02  1591.735352  1603.569815  1573.983656  1591.735352   9006000   \n",
      "2017-08-03  1597.652696  1600.611312  1573.983767  1576.942383   3998000   \n",
      "2017-08-04  1588.776985  1609.487299  1582.859752  1600.611450   8172500   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for BBCA:\n",
      "                   Open         High          Low        Close      Volume  \\\n",
      "Date                                                                         \n",
      "2017-07-31  3435.631645  3477.191706  3431.013861  3454.102783  65311000.0   \n",
      "2017-08-01  3486.427273  3504.898411  3458.720566  3495.662842  88810000.0   \n",
      "2017-08-02  3495.662842  3504.898411  3477.191704  3495.662842  62681500.0   \n",
      "2017-08-03  3518.751793  3518.751793  3426.396103  3463.338379  43917500.0   \n",
      "2017-08-04  3463.338654  3463.338654  3426.396375  3431.014160  70007000.0   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for BBNI:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  6600.279864  6690.079590  6532.930069  6690.079590  19748500   \n",
      "2017-08-01  6667.629997  6667.629997  6510.480469  6510.480469  17418900   \n",
      "2017-08-02  6532.930176  6555.380108  6420.680516  6532.930176  10770200   \n",
      "2017-08-03  6510.480659  6555.380526  6443.130859  6443.130859   6497900   \n",
      "2017-08-04  6510.480244  6555.380108  6488.030312  6532.930176  11087100   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for BBTN:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  2457.620006  2467.108887  2429.153365  2467.108887  14907400   \n",
      "2017-08-01  2467.108667  2505.064185  2448.130908  2486.086426  20624300   \n",
      "2017-08-02  2505.064453  2514.553334  2457.620051  2505.064453   9523300   \n",
      "2017-08-03  2505.064296  2505.064296  2457.619896  2476.597656   6460100   \n",
      "2017-08-04  2467.108931  2505.064453  2438.642290  2505.064453  12632500   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for BMRI:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  2457.620006  2467.108887  2429.153365  2467.108887  14907400   \n",
      "2017-08-01  2467.108667  2505.064185  2448.130908  2486.086426  20624300   \n",
      "2017-08-02  2505.064453  2514.553334  2457.620051  2505.064453   9523300   \n",
      "2017-08-03  2505.064296  2505.064296  2457.619896  2476.597656   6460100   \n",
      "2017-08-04  2467.108931  2505.064453  2438.642290  2505.064453  12632500   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for BBRI:\n",
      "                   Open         High          Low        Close     Volume  \\\n",
      "Date                                                                        \n",
      "2017-07-31  2472.547789  2571.106887  2472.547789  2532.540283  224232500   \n",
      "2017-08-01  2553.966398  2553.966398  2532.540505  2536.825684   86513500   \n",
      "2017-08-02  2562.536621  2562.536621  2532.540373  2562.536621   53040000   \n",
      "2017-08-03  2566.821888  2566.821888  2536.825639  2545.395996  101815500   \n",
      "2017-08-04  2545.395930  2562.536643  2545.395930  2558.251465   57858500   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for KLBF:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  1559.695853  1573.297852  1532.491855  1573.297852  24428400   \n",
      "2017-08-01  1573.297817  1582.365815  1532.491821  1532.491821  29328600   \n",
      "2017-08-02  1568.763794  1582.365792  1546.093797  1568.763794  48417600   \n",
      "2017-08-03  1559.695853  1595.967849  1559.695853  1573.297852  42387400   \n",
      "2017-08-04  1573.297828  1614.103824  1573.297828  1609.569824  54651000   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31          0             0  \n",
      "2017-08-01          0             0  \n",
      "2017-08-02          0             0  \n",
      "2017-08-03          0             0  \n",
      "2017-08-04          0             0  \n",
      "Data for MIKA:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  2063.928327  2149.925341  2044.817880  2140.370117  11915200   \n",
      "2017-08-01  2130.814990  2188.146335  2121.259766  2121.259766   9301400   \n",
      "2017-08-02  2140.370117  2169.035788  2102.149222  2140.370117  13742000   \n",
      "2017-08-03  2140.370264  2178.591162  2092.594142  2159.480713  16928200   \n",
      "2017-08-04  2159.481093  2255.033354  2149.925867  2226.367676  12690400   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31          0             0  \n",
      "2017-08-01          0             0  \n",
      "2017-08-02          0             0  \n",
      "2017-08-03          0             0  \n",
      "2017-08-04          0             0  \n",
      "Data for ANTM:\n",
      "                  Open        High         Low       Close    Volume  \\\n",
      "Date                                                                   \n",
      "2017-07-31  663.753907  668.529115  654.203491  658.978699  19661600   \n",
      "2017-08-01  668.529115  668.529115  654.203491  658.978699  29985500   \n",
      "2017-08-02  654.203491  658.978699  649.428283  654.203491  32474400   \n",
      "2017-08-03  658.978700  668.529116  649.428284  649.428284  25897800   \n",
      "2017-08-04  649.428348  654.203557  620.777098  630.327515  61558600   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for BRPT:\n",
      "                  Open        High         Low       Close     Volume  \\\n",
      "Date                                                                    \n",
      "2017-07-31  337.541509  337.541509  324.859314  324.859314  176012500   \n",
      "2017-08-01  326.810478  341.443783  324.859371  339.492676  284122500   \n",
      "2017-08-02  347.297028  347.297028  337.541493  347.297028  168621000   \n",
      "2017-08-03  348.272613  366.808130  347.297060  363.881470  360305500   \n",
      "2017-08-04  365.832600  370.710368  358.028171  359.979279  154226000   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for INCO:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  2476.813703  2486.720957  2387.648409  2397.555664   7944600   \n",
      "2017-08-01  2407.463077  2456.999354  2357.926800  2427.277588  15252600   \n",
      "2017-08-02  2407.462891  2447.091909  2397.555636  2407.462891   5817100   \n",
      "2017-08-03  2437.184655  2466.906419  2397.555636  2407.462891   6212800   \n",
      "2017-08-04  2407.463048  2437.184814  2367.834027  2437.184814   4645900   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for INKP:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  2613.899656  2671.771973  2488.509635  2671.771973  13056400   \n",
      "2017-08-01  2662.126711  2671.772097  2604.254391  2642.835938   9896700   \n",
      "2017-08-02  2584.963379  2642.835693  2584.963379  2584.963379   3818800   \n",
      "2017-08-03  2594.608765  2594.608765  2517.445679  2565.672607   7046000   \n",
      "2017-08-04  2575.317993  2575.317993  2536.736450  2565.672607   1375300   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31          0             0  \n",
      "2017-08-01          0             0  \n",
      "2017-08-02          0             0  \n",
      "2017-08-03          0             0  \n",
      "2017-08-04          0             0  \n",
      "Data for MDKA:\n",
      "                  Open        High         Low       Close  Volume  Dividends  \\\n",
      "Date                                                                            \n",
      "2017-07-31  441.224579  458.489868  441.224579  458.489868    4691          0   \n",
      "2017-08-01  458.489868  458.489868  458.489868  458.489868       0          0   \n",
      "2017-08-02  470.000092  470.000092  468.081726  470.000092   21372          0   \n",
      "2017-08-03  479.591919  479.591919  412.449066  441.224579   97478          0   \n",
      "2017-08-04  460.408234  460.408234  460.408234  460.408234   83404          0   \n",
      "\n",
      "            Stock Splits  \n",
      "Date                      \n",
      "2017-07-31             0  \n",
      "2017-08-01             0  \n",
      "2017-08-02             0  \n",
      "2017-08-03             0  \n",
      "2017-08-04             0  \n",
      "Data for SMGR:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  9164.627930  9164.627930  8980.414303  9164.627930   5371700   \n",
      "2017-08-01  9187.653008  9187.653008  9072.519512  9141.599609   3680600   \n",
      "2017-08-02  9855.428711  9901.482116  9118.574228  9855.428711  20325300   \n",
      "2017-08-03  9786.347703  9855.427804  9602.134099  9648.187500   6705700   \n",
      "2017-08-04  9533.055023  9786.348756  9487.001617  9510.028320   4300300   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for TINS:\n",
      "                  Open        High         Low       Close    Volume  \\\n",
      "Date                                                                   \n",
      "2017-07-31  771.838857  776.488488  743.941067  748.590698  20591700   \n",
      "2017-08-01  725.342574  771.838892  725.342574  743.941101  13896300   \n",
      "2017-08-02  729.992188  748.590715  720.692924  729.992188  14881400   \n",
      "2017-08-03  734.641837  762.539629  734.641837  743.941101  22934900   \n",
      "2017-08-04  743.941119  757.890015  743.941119  757.890015  16066500   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for UNTR:\n",
      "                    Open          High           Low         Close   Volume  \\\n",
      "Date                                                                          \n",
      "2017-07-31  23998.982753  24568.450140  23876.954027  24487.097656  4072600   \n",
      "2017-08-01  24649.800903  25137.915772  24649.800903  24812.505859  4369700   \n",
      "2017-08-02  24731.156250  24812.508738  24019.321983  24731.156250  1767100   \n",
      "2017-08-03  24405.745666  24405.745666  23754.925781  23754.925781  1957900   \n",
      "2017-08-04  23795.602024  24182.026330  23470.192082  23754.925781  2089400   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31          0             0  \n",
      "2017-08-01          0             0  \n",
      "2017-08-02          0             0  \n",
      "2017-08-03          0             0  \n",
      "2017-08-04          0             0  \n",
      "Data for ASII:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  6814.431546  6964.907952  6792.934916  6857.424805  57540900   \n",
      "2017-08-01  6857.424550  6857.424550  6706.948149  6749.941406  75196300   \n",
      "2017-08-02  6728.444336  6792.934218  6642.457827  6728.444336  61600300   \n",
      "2017-08-03  6728.444427  6728.444427  6642.457917  6685.451172  39479300   \n",
      "2017-08-04  6663.954544  6706.947799  6577.968034  6685.451172  62576700   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for BUKA:\n",
      "            Open  High   Low  Close      Volume  Dividends  Stock Splits\n",
      "Date                                                                    \n",
      "2021-08-06  1055  1060  1055   1060   524138200          0             0\n",
      "2021-08-09  1325  1325  1110   1110  3578780800          0             0\n",
      "2021-08-10  1130  1160  1035   1035   994505400          0             0\n",
      "2021-08-12   965  1000   965    965  2751889000          0             0\n",
      "2021-08-13   940  1035   910    955  3239118200          0             0\n",
      "Data for EMTK:\n",
      "                   Open         High          Low        Close  Volume  \\\n",
      "Date                                                                     \n",
      "2017-07-31  1144.949951  1144.949951  1144.949951  1144.949951       0   \n",
      "2017-08-01  1144.949951  1144.949951  1144.949951  1144.949951       0   \n",
      "2017-08-02  1144.949951  1144.949951  1144.949951  1144.949951       0   \n",
      "2017-08-03  1144.949951  1144.949951  1144.949951  1144.949951       0   \n",
      "2017-08-04  1144.949951  1144.949951  1144.949951  1144.949951       0   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31          0             0  \n",
      "2017-08-01          0             0  \n",
      "2017-08-02          0             0  \n",
      "2017-08-03          0             0  \n",
      "2017-08-04          0             0  \n",
      "Data for GOTO:\n",
      "            Open  High  Low  Close      Volume  Dividends  Stock Splits\n",
      "Date                                                                   \n",
      "2022-04-11   400   416  372    382  9410897000          0             0\n",
      "2022-04-12   422   442  360    370  3887331000          0             0\n",
      "2022-04-13   370   380  360    374  3262811400          0             0\n",
      "2022-04-14   374   382  374    376  3675981900          0             0\n",
      "2022-04-18   376   380  370    378  2660312700          0             0\n",
      "Data for EXCL:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  3169.134260  3236.358320  3159.530823  3226.754883  12005900   \n",
      "2017-08-01  3226.754773  3341.996015  3217.151336  3332.392578  13252200   \n",
      "2017-08-02  3457.237305  3486.047616  3351.599498  3457.237305  15130200   \n",
      "2017-08-03  3447.633876  3457.237313  3428.427002  3428.427002   5270000   \n",
      "2017-08-04  3438.030564  3438.030564  3293.979004  3293.979004   1748000   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for TBIG:\n",
      "                   Open         High          Low        Close    Volume  \\\n",
      "Date                                                                       \n",
      "2017-07-31  1193.469376  1211.085160  1184.661484  1189.065430  10869500   \n",
      "2017-08-01  1197.873322  1215.489106  1180.257538  1189.065430  15900000   \n",
      "2017-08-02  1206.681274  1206.681274  1189.065489  1206.681274   7607000   \n",
      "2017-08-03  1206.681151  1206.681151  1171.449585  1171.449585   4379000   \n",
      "2017-08-04  1171.449570  1197.873244  1167.045624  1175.853516   3586000   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for TLKM:\n",
      "                   Open         High          Low        Close     Volume  \\\n",
      "Date                                                                        \n",
      "2017-07-31  3998.737184  4058.547355  3990.192874  4007.281494  105620500   \n",
      "2017-08-01  4015.826777  4118.358524  4007.282465  4075.636963   98006400   \n",
      "2017-08-02  4101.269531  4135.446777  4084.180908  4101.269531   76646000   \n",
      "2017-08-03  4067.092038  4092.724971  3990.193239  4015.826172   57453500   \n",
      "2017-08-04  4015.826539  4024.370851  3981.649292  3990.193604   53596700   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Data for TOWR:\n",
      "                  Open        High         Low       Close    Volume  \\\n",
      "Date                                                                   \n",
      "2017-07-31  684.519962  691.365162  682.808662  686.231262  25836500   \n",
      "2017-08-01  684.519936  701.632935  677.674737  701.632935  39488000   \n",
      "2017-08-02  694.787659  694.787659  687.942460  694.787659  29044000   \n",
      "2017-08-03  694.787805  735.859005  694.787805  734.147705  42867000   \n",
      "2017-08-04  730.725098  734.147698  727.302498  730.725098  17839500   \n",
      "\n",
      "            Dividends  Stock Splits  \n",
      "Date                                 \n",
      "2017-07-31        0.0             0  \n",
      "2017-08-01        0.0             0  \n",
      "2017-08-02        0.0             0  \n",
      "2017-08-03        0.0             0  \n",
      "2017-08-04        0.0             0  \n",
      "Columns: ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "Data preview:\n",
      "Merged Stock Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table id=\"itables_9eb3d9c5_fe17_4aeb_8818_b7f2f99237ef\" class=\"display nowrap\" data-quarto-disable-processing=\"true\" style=\"table-layout:auto;width:auto;margin:auto;caption-side:bottom\">\n",
       "<thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPIN</th>\n",
       "      <th>ICBP</th>\n",
       "      <th>INDF</th>\n",
       "      <th>UNVR</th>\n",
       "      <th>ADRO</th>\n",
       "      <th>PGAS</th>\n",
       "      <th>PTBA</th>\n",
       "      <th>BBCA</th>\n",
       "      <th>BBNI</th>\n",
       "      <th>BBTN</th>\n",
       "      <th>BMRI</th>\n",
       "      <th>BBRI</th>\n",
       "      <th>KLBF</th>\n",
       "      <th>MIKA</th>\n",
       "      <th>ANTM</th>\n",
       "      <th>BRPT</th>\n",
       "      <th>INCO</th>\n",
       "      <th>INKP</th>\n",
       "      <th>MDKA</th>\n",
       "      <th>SMGR</th>\n",
       "      <th>TINS</th>\n",
       "      <th>UNTR</th>\n",
       "      <th>ASII</th>\n",
       "      <th>BUKA</th>\n",
       "      <th>EMTK</th>\n",
       "      <th>GOTO</th>\n",
       "      <th>EXCL</th>\n",
       "      <th>TBIG</th>\n",
       "      <th>TLKM</th>\n",
       "      <th>TOWR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead><tbody><tr>\n",
       "<td style=\"vertical-align:middle; text-align:left\">\n",
       "<a href=https://mwouts.github.io/itables/><svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
       "width=\"64\" viewBox=\"0 0 500 400\" style=\"font-family: 'Droid Sans', sans-serif;\">\n",
       "    <g style=\"fill:#d9d7fc\">\n",
       "        <path d=\"M100,400H500V357H100Z\" />\n",
       "        <path d=\"M100,300H400V257H100Z\" />\n",
       "        <path d=\"M0,200H400V157H0Z\" />\n",
       "        <path d=\"M100,100H500V57H100Z\" />\n",
       "        <path d=\"M100,350H500V307H100Z\" />\n",
       "        <path d=\"M100,250H400V207H100Z\" />\n",
       "        <path d=\"M0,150H400V107H0Z\" />\n",
       "        <path d=\"M100,50H500V7H100Z\" />\n",
       "    </g>\n",
       "    <g style=\"fill:#1a1366;stroke:#1a1366;\">\n",
       "   <rect x=\"100\" y=\"7\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"0\" y=\"107\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"0;0;400\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"207\" width=\"300\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;300;0\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;400\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"307\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <g style=\"fill:transparent;stroke-width:8; stroke-linejoin:round\" rx=\"5\">\n",
       "            <g transform=\"translate(45 50) rotate(-45)\">\n",
       "                <circle r=\"33\" cx=\"0\" cy=\"0\" />\n",
       "                <rect x=\"-8\" y=\"32\" width=\"16\" height=\"30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(450 152)\">\n",
       "                <polyline points=\"-15,-20 -35,-20 -35,40 25,40 25,20\" />\n",
       "                <rect x=\"-15\" y=\"-40\" width=\"60\" height=\"60\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(50 352)\">\n",
       "                <polygon points=\"-35,-5 0,-40 35,-5\" />\n",
       "                <polygon points=\"-35,10 0,45 35,10\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(75 250)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(425 250) rotate(180)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "        </g>\n",
       "    </g>\n",
       "</svg>\n",
       "</a>\n",
       "Loading ITables v2.3.0 from the internet...\n",
       "(need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>\n",
       "</tr></tbody>\n",
       "</table>\n",
       "<link href=\"https://www.unpkg.com/dt_for_itables@2.2.0/dt_bundle.css\" rel=\"stylesheet\">\n",
       "<script type=\"module\">\n",
       "    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.2.0/dt_bundle.js';\n",
       "\n",
       "    document.querySelectorAll(\"#itables_9eb3d9c5_fe17_4aeb_8818_b7f2f99237ef:not(.dataTable)\").forEach(table => {\n",
       "        if (!(table instanceof HTMLTableElement))\n",
       "            return;\n",
       "\n",
       "        // Define the table data\n",
       "        const data = [[\"2022-04-11\", 5292.0, 7525.0, 6075.0, 3401.40332, 3001.693115, 1260.020264, 3022.088867, 7725.0, 8400.0, 1620.0, 1620.0, 4570.0, 1550.966309, 2388.885742, 2589.249023, 866.667542, 7300.0, 7725.160156, 5073.125977, 6350.0, 1802.631714, 27580.160156, 6674.567383, 328.0, 2710.0, 382.0, 2535.993164, 2862.714355, 4428.037109, 991.250244], [\"2022-04-12\", 5218.5, 7400.0, 6150.0, 3460.387207, 3001.693115, 1232.123047, 2988.037354, 7800.0, 8425.0, 1630.0, 1630.0, 4610.0, 1550.966309, 2438.24292, 2628.629395, 881.610046, 7550.0, 7750.0, 5365.806641, 6300.0, 1831.706421, 27095.447266, 6772.006348, 322.0, 2710.0, 370.0, 2526.163818, 2902.199951, 4466.625977, 991.250244], [\"2022-04-13\", 5120.5, 7325.0, 6200.0, 3381.741943, 3202.443359, 1264.6698, 3200.860352, 7800.0, 8450.0, 1635.0, 1635.0, 4610.0, 1560.751587, 2467.857178, 2746.77002, 871.648376, 7425.0, 7650.641113, 5390.196777, 6200.0, 1870.472778, 28379.9375, 6796.366211, 348.0, 2700.0, 374.0, 2673.605225, 3079.885742, 4524.508789, 1015.786133], [\"2022-04-14\", 4890.200195, 7175.0, 6125.0, 3342.419434, 3164.205322, 1260.020264, 3209.373291, 7700.0, 8400.0, 1620.0, 1620.0, 4540.0, 1550.966309, 2576.442871, 2736.924805, 866.667542, 7975.0, 7576.12207, 5250.0, 6025.0, 1860.781128, 28888.884766, 6698.927246, 346.0, 2740.0, 376.0, 2663.775879, 3050.271484, 4514.861328, 991.250244], [\"2022-04-18\", 4900.0, 7100.0, 6125.0, 3332.588867, 3145.086182, 1250.721191, 3200.860352, 7700.0, 8425.0, 1610.0, 1610.0, 4550.0, 1565.644165, 2665.285645, 2855.06543, 856.705811, 8300.0, 7551.282227, 5700.0, 6150.0, 1986.771606, 29082.769531, 6820.726074, 326.0, 2700.0, 378.0, 2889.852783, 3050.271484, 4543.802734, 991.250244], [\"2022-04-19\", 5047.0, 7175.0, 6200.0, 3293.266113, 3125.967041, 1301.865967, 3192.347412, 7625.0, 8400.0, 1630.0, 1630.0, 4570.0, 1541.18103, 2615.928467, 2805.840332, 881.610046, 8675.0, 7551.282227, 5425.0, 6050.0, 1948.005249, 28695.0, 6723.287109, 330.0, 2740.0, 358.0, 2899.682129, 3020.657227, 4514.861328, 996.15741], [\"2022-04-20\", 4924.5, 7325.0, 6225.0, 3411.233887, 3087.729004, 1292.566895, 3149.782959, 7650.0, 8975.0, 1665.0, 1665.0, 4850.0, 1526.503052, 2655.414307, 2776.305176, 866.667542, 8475.0, 7551.282227, 5700.0, 5950.0, 1928.622192, 28350.0, 6723.287109, 368.0, 2720.0, 338.0, 3125.759033, 3010.785645, 4466.625977, 1025.600464], [\"2022-04-21\", 4900.0, 7425.0, 6250.0, 3342.419434, 3125.967041, 1306.515381, 3166.808838, 7925.0, 9375.0, 1760.0, 1760.0, 4800.0, 1565.644165, 2665.285645, 2746.77002, 886.590942, 8000.0, 7551.282227, 5500.0, 6075.0, 1904.393188, 28125.0, 6820.726074, 372.0, 2820.0, 340.0, 3184.735596, 2991.042969, 4456.978516, 1030.50769], [\"2022-04-22\", 4900.0, 7300.0, 6300.0, 3371.911377, 3068.609863, 1287.917358, 3141.27002, 7875.0, 9350.0, 1775.0, 1775.0, 4750.0, 1541.18103, 2566.571533, 2697.544678, 871.648376, 7950.0, 7501.602539, 5300.0, 6000.0, 1875.318481, 28300.0, 6869.445312, 370.0, 2790.0, 340.0, 3106.100342, 2991.042969, 4456.978516, 1010.878967], [\"2022-04-25\", 4821.600098, 7425.0, 6300.0, 3627.508057, 3068.609863, 1255.370728, 3149.782959, 8000.0, 9500.0, 1865.0, 1865.0, 4840.0, 1570.536865, 2586.314209, 2579.404053, 861.686646, 7725.0, 7451.92334, 5200.0, 5975.0, 1817.169067, 27625.0, 6893.805176, 362.0, 2870.0, 328.0, 3233.882812, 3040.399902, 4543.802734, 1010.878967], [\"2022-04-26\", 4924.5, 7575.0, 6300.0, 3824.121094, 3020.812256, 1255.370728, 3098.705322, 8125.0, 9600.0, 1835.0, 1835.0, 4940.0, 1590.1073, 2546.828613, 2461.263428, 866.667542, 7200.0, 7352.564453, 5075.0, 6400.0, 1768.711304, 28750.0, 7015.604004, 364.0, 2900.0, 310.0, 3145.417969, 3040.399902, 4592.038574, 1010.878967], [\"2022-04-27\", 4949.0, 7600.0, 6350.0, 3745.47583, 3030.371826, 1273.96875, 3149.782959, 8200.0, 9450.0, 1835.0, 1835.0, 4850.0, 1580.322144, 2517.214355, 2520.33374, 861.686646, 7175.0, 7377.403809, 5300.0, 6475.0, 1807.477539, 28900.0, 6942.524902, 372.0, 3000.0, 290.0, 3125.759033, 3020.657227, 4601.685547, 996.15741], [\"2022-04-28\", 5047.0, 7625.0, 6300.0, 3824.121094, 3192.883789, 1348.361084, 3251.937988, 8125.0, 9225.0, 1845.0, 1845.0, 4870.0, 1604.785278, 2497.471436, 2559.713867, 846.744141, 7300.0, 7526.442383, 5300.0, 6400.0, 1797.786011, 30275.0, 7381.0, 382.0, 2990.0, 272.0, 3135.588623, 2971.300049, 4456.978516, 991.250244], [\"2022-05-09\", 4831.399902, 7400.0, 6125.0, 3942.088867, 3020.812256, 1348.361084, 3209.373291, 7600.0, 8825.0, 1720.0, 1720.0, 4530.0, 1497.147217, 2418.5, 2490.798584, 816.85907, 7175.0, 7278.044922, 4950.0, 6125.0, 1729.944946, 29225.0, 7050.0, 356.0, 2790.0, 254.0, 2988.146973, 2971.300049, 4157.91748, 951.992798], [\"2022-05-10\", 4973.5, 7975.0, 6400.0, 4305.822754, 2953.895508, 1408.804932, 3115.731201, 7525.0, 8900.0, 1700.0, 1700.0, 4530.0, 1580.322144, 2457.98584, 2382.50293, 821.839905, 6800.0, 7451.92334, 4830.0, 6100.0, 1647.56665, 29625.0, 7000.0, 332.0, 2600.0, 238.0, 2939.0, 2921.942871, 4186.858887, 947.085632], [\"2022-05-11\", 4973.5, 8300.0, 6450.0, 4797.35498, 3011.252686, 1413.454468, 3158.295898, 7650.0, 8775.0, 1680.0, 1680.0, 4490.0, 1599.892578, 2477.728516, 2362.812744, 846.744141, 6775.0, 7551.282227, 4860.0, 6475.0, 1657.258179, 30225.0, 6900.0, 310.0, 2560.0, 222.0, 2990.0, 3020.657227, 4177.211426, 996.15741], [\"2022-05-12\", 4743.200195, 8025.0, 6400.0, 4610.572754, 3011.252686, 1366.959229, 3107.218262, 7275.0, 8200.0, 1630.0, 1630.0, 4320.0, 1546.073608, 2655.414307, 2293.897461, 791.954834, 6650.0, 7228.365723, 4600.0, 6275.0, 1579.725708, 29300.0, 6850.0, 290.0, 2390.0, 208.0, 2790.0, 2902.199951, 4148.27002, 956.900024], [\"2022-05-13\", 4949.0, 8175.0, 6450.0, 4718.709961, 3068.609863, 1390.206787, 3234.912109, 7325.0, 8250.0, 1645.0, 1645.0, 4250.0, 1565.644165, 2625.800049, 2293.897461, 801.916504, 6900.0, 7278.044922, 4380.0, 6300.0, 1584.571411, 30225.0, 7125.0, 272.0, 2230.0, 194.0, 2850.0, 2931.814209, 4109.681641, 976.528687], [\"2022-05-17\", 4831.399902, 7825.0, 6500.0, 4718.709961, 3220.0, 1450.650635, 3490.300049, 7400.0, 8450.0, 1665.0, 1665.0, 4450.0, 1590.1073, 2625.800049, 2352.967773, 821.839905, 7275.0, 7625.80127, 4480.0, 6350.0, 1623.337769, 30075.0, 7000.0, 282.0, 2080.0, 200.0, 2790.0, 2842.971436, 4032.504395, 956.900024], [\"2022-05-18\", 4851.0, 8100.0, 6500.0, 4939.899414, 3190.0, 1464.599121, 3405.170654, 7575.0, 8750.0, 1675.0, 1675.0, 4450.0, 1590.1073, 2685.028564, 2362.812744, 821.839905, 7100.0, 7650.641113, 4750.0, 6475.0, 1618.491943, 30300.0, 7100.0, 304.0, 1935.0, 248.0, 2710.0, 2862.714355, 4100.034668, 966.714355], [\"2022-05-19\", 4870.600098, 7975.0, 6450.0, 4797.35498, 3110.0, 1446.001099, 3362.605957, 7450.0, 8675.0, 1665.0, 1665.0, 4440.0, 1570.536865, 2704.771484, 2421.883057, 801.916504, 7625.0, 7476.762695, 4980.0, 6225.0, 1652.412476, 29975.0, 7000.0, 296.0, 1940.0, 280.0, 2660.0, 2803.48584, 4061.445801, 942.178467], [\"2022-05-20\", 5047.0, 8050.0, 6450.0, 4964.476074, 3310.0, 1464.599121, 3447.735352, 7400.0, 8775.0, 1660.0, 1660.0, 4430.0, 1590.1073, 2764.0, 2451.418213, 811.878174, 7975.0, 7576.12207, 4860.0, 6350.0, 1686.332886, 29900.0, 7125.0, 316.0, 2000.0, 304.0, 2690.0, 2734.385742, 4022.857422, 942.178467], [\"2022-05-23\", 4860.800293, 7975.0, 6400.0, 4797.35498, 3150.0, 1446.001099, 3490.300049, 7375.0, 8850.0, 1655.0, 1655.0, 4330.0, 1595.0, 3010.0, 2490.798584, 815.0, 7725.0, 7774.839844, 4840.0, 6500.0, 1696.024536, 29650.0, 6950.0, 294.0, 1895.0, 296.0, 2600.0, 2754.128662, 3964.974609, 932.364075], [\"2022-05-24\", 4890.200195, 8150.0, 6275.0, 4728.540527, 3200.0, 1576.187622, 3745.687744, 7350.0, 9075.0, 1680.0, 1680.0, 4460.0, 1580.322144, 3050.0, 2530.178711, 815.0, 7850.0, 7725.160156, 5275.0, 6700.0, 1749.328125, 30300.0, 7150.0, 280.0, 1875.0, 302.0, 2600.0, 2744.25708, 4003.562988, 927.456909], [\"2022-05-25\", 4782.399902, 8250.0, 6400.0, 4689.217773, 3100.0, 1650.579956, 3737.174805, 7375.0, 8925.0, 1675.0, 1675.0, 4350.0, 1595.0, 3050.0, 2451.418213, 815.0, 7600.0, 7625.80127, 5100.0, 6525.0, 1715.407593, 30175.0, 7100.0, 282.0, 1825.0, 304.0, 2570.0, 2754.128662, 4051.798828, 917.642578], [\"2022-05-27\", 4821.600098, 8625.0, 6425.0, 4679.387207, 3160.0, 1655.229492, 3762.713623, 7575.0, 9200.0, 1680.0, 1680.0, 4540.0, 1575.429443, 2840.0, 2431.728271, 840.0, 7800.0, 7675.480957, 5275.0, 6700.0, 1715.407593, 30400.0, 7225.0, 286.0, 1865.0, 312.0, 2670.0, 2724.514404, 4148.27002, 947.085632], [\"2022-05-30\", 4998.0, 8550.0, 6450.0, 4571.25, 3060.0, 1655.229492, 3771.226562, 7575.0, 9200.0, 1675.0, 1675.0, 4430.0, 1595.0, 2650.0, 2520.33374, 825.0, 8025.0, 7998.397461, 5175.0, 7175.0, 1754.17395, 30250.0, 7250.0, 302.0, 1875.0, 314.0, 2690.0, 2744.25708, 4100.034668, 947.085632], [\"2022-05-31\", 5292.0, 8575.0, 6600.0, 4649.89502, 3270.0, 1673.827637, 3856.355713, 7750.0, 9175.0, 1700.0, 1700.0, 4630.0, 1675.0, 2610.0, 2471.108398, 835.0, 8225.0, 8197.115234, 5450.0, 7300.0, 1841.398071, 31300.0, 7350.0, 294.0, 1915.0, 304.0, 2640.0, 2912.071533, 4157.91748, 981.435913], [\"2022-06-02\", 5292.0, 8550.0, 6700.0, 4708.878906, 3330.0, 1659.879028, 3873.381592, 7575.0, 8850.0, 1675.0, 1675.0, 4480.0, 1605.0, 2690.0, 2471.108398, 825.0, 7925.0, 8370.994141, 5200.0, 7175.0, 1933.467896, 31625.0, 7325.0, 306.0, 1865.0, 344.0, 2680.0, 2764.0, 4138.623047, 961.80719], [\"2022-06-03\", 5275.0, 8575.0, 6675.0, 4640.064453, 3520.0, 1664.528564, 3941.485107, 7600.0, 8950.0, 1675.0, 1675.0, 4520.0, 1600.0, 2510.0, 2461.263428, 840.0, 7675.0, 8221.955078, 5200.0, 7100.0, 1923.776367, 32600.0, 7300.0, 304.0, 1860.0, 354.0, 2640.0, 2790.0, 4157.91748, 956.900024], [\"2022-06-06\", 5350.0, 8650.0, 6925.0, 4689.217773, 3460.0, 1659.879028, 4310.0, 7450.0, 8900.0, 1685.0, 1685.0, 4430.0, 1640.0, 2690.0, 2450.0, 830.0, 7625.0, 8122.596191, 5300.0, 7200.0, 1925.0, 32175.0, 7225.0, 284.0, 1770.0, 330.0, 2570.0, 2800.0, 4186.858887, 966.714355], [\"2022-06-07\", 5200.0, 8575.0, 6925.0, 4649.89502, 3650.0, 1650.579956, 4070.0, 7375.0, 8900.0, 1670.0, 1670.0, 4400.0, 1595.0, 2580.0, 2420.0, 810.0, 7650.0, 8147.436035, 5150.0, 7150.0, 1950.0, 32825.0, 7200.0, 280.0, 1745.0, 364.0, 2590.0, 2800.0, 4100.034668, 956.900024], [\"2022-06-08\", 5150.0, 8575.0, 6975.0, 4807.185547, 3630.0, 1690.0, 4050.0, 7600.0, 9000.0, 1625.0, 1625.0, 4470.0, 1625.0, 2700.0, 2420.0, 810.0, 7700.0, 8296.474609, 5125.0, 7225.0, 1960.0, 33050.0, 7175.0, 288.0, 1815.0, 378.0, 2670.0, 2840.0, 4100.0, 965.0], [\"2022-06-09\", 4950.0, 8625.0, 6875.0, 4738.371094, 3600.0, 1680.0, 4070.0, 7500.0, 8675.0, 1625.0, 1625.0, 4490.0, 1635.0, 2620.0, 2400.0, 805.0, 7650.0, 8221.955078, 5150.0, 7000.0, 1985.0, 32400.0, 7200.0, 290.0, 1790.0, 384.0, 2600.0, 2810.0, 4050.0, 940.0], [\"2022-06-10\", 5025.0, 8500.0, 6875.0, 4669.556641, 3430.0, 1680.0, 3920.0, 7350.0, 8300.0, 1560.0, 1560.0, 4400.0, 1625.0, 2620.0, 2290.0, 775.0, 7400.0, 8321.314453, 4970.0, 6900.0, 1930.0, 32200.0, 6925.0, 280.0, 1775.0, 386.0, 2560.0, 2810.0, 4020.0, 920.0], [\"2022-06-13\", 5025.0, 8600.0, 6825.0, 4610.572754, 3190.0, 1640.0, 3870.0, 7350.0, 8250.0, 1470.0, 1470.0, 4340.0, 1625.0, 2700.0, 2260.0, 745.0, 7200.0, 8147.436035, 4700.0, 6550.0, 1840.0, 30675.0, 7025.0, 268.0, 1665.0, 388.0, 2410.0, 2870.0, 4080.0, 895.0], [\"2022-06-14\", 5075.0, 8650.0, 6875.0, 4659.726074, 3260.0, 1730.0, 3880.0, 7400.0, 8275.0, 1490.0, 1490.0, 4360.0, 1625.0, 2700.0, 2180.0, 720.0, 7200.0, 7948.718262, 4700.0, 6825.0, 1860.0, 31550.0, 6975.0, 270.0, 1640.0, 400.0, 2440.0, 2900.0, 4110.0, 890.0], [\"2022-06-15\", 5025.0, 8600.0, 6800.0, 4718.709961, 3040.0, 1715.0, 3680.0, 7325.0, 8275.0, 1490.0, 1490.0, 4450.0, 1615.0, 2680.0, 2170.0, 725.0, 6925.0, 7750.0, 4430.0, 6900.0, 1820.0, 29750.0, 6700.0, 278.0, 1755.0, 404.0, 2410.0, 2910.0, 4060.0, 880.0], [\"2022-06-16\", 5150.0, 8675.0, 6800.0, 4718.709961, 3060.0, 1820.0, 3820.0, 7575.0, 8250.0, 1500.0, 1500.0, 4430.0, 1625.0, 2680.0, 2150.0, 745.0, 7100.0, 8048.077148, 4400.0, 6900.0, 1790.0, 30000.0, 6850.0, 278.0, 1730.0, 392.0, 2380.0, 2950.0, 4040.0, 915.0], [\"2022-06-17\", 5025.0, 8550.0, 6825.0, 4571.25, 2920.0, 1800.0, 3730.0, 7500.0, 8175.0, 1480.0, 1480.0, 4370.0, 1625.0, 2700.0, 2110.0, 720.0, 6900.0, 7874.19873, 4100.0, 6825.0, 1720.0, 28800.0, 6950.0, 284.0, 1760.0, 366.0, 2330.0, 2920.0, 4120.0, 915.0], [\"2022-06-20\", 5175.0, 8975.0, 6900.0, 4939.899414, 2930.0, 1775.0, 3730.0, 7625.0, 8125.0, 1465.0, 1465.0, 4400.0, 1620.0, 2880.0, 2070.0, 750.0, 6650.0, 7750.0, 4550.0, 7100.0, 1685.0, 29000.0, 6925.0, 284.0, 1735.0, 370.0, 2320.0, 2950.0, 4040.0, 940.0], [\"2022-06-21\", 5325.0, 9150.0, 6875.0, 4964.476074, 3030.0, 1770.0, 4030.0, 7650.0, 8450.0, 1485.0, 1485.0, 4380.0, 1625.0, 2840.0, 2150.0, 770.0, 7050.0, 7775.0, 4600.0, 7150.0, 1735.0, 29150.0, 6850.0, 298.0, 1750.0, 378.0, 2500.0, 2940.0, 4110.0, 950.0], [\"2022-06-22\", 5450.0, 9225.0, 6975.0, 4807.185547, 3030.0, 1765.0, 4120.0, 7500.0, 8150.0, 1490.0, 1490.0, 4320.0, 1625.0, 2780.0, 2020.0, 750.0, 6750.0, 7775.0, 4490.0, 7150.0, 1615.0, 29200.0, 6800.0, 286.0, 1695.0, 380.0, 2480.0, 2850.0, 4050.0, 960.0], [\"2022-06-23\", 5650.0, 9325.0, 7125.0, 4876.0, 2940.0, 1725.0, 4090.0, 7525.0, 8150.0, 1480.0, 1480.0, 4310.0, 1700.0, 2750.0, 1965.0, 765.0, 6400.0, 7750.0, 4440.0, 7300.0, 1510.0, 29275.0, 6875.0, 280.0, 1665.0, 380.0, 2670.0, 2900.0, 4160.0, 950.0], [\"2022-06-24\", 5750.0, 9425.0, 7075.0, 4990.0, 2970.0, 1700.0, 4090.0, 7475.0, 8150.0, 1485.0, 1485.0, 4340.0, 1710.0, 2620.0, 1970.0, 750.0, 6300.0, 7725.0, 4440.0, 7625.0, 1485.0, 29275.0, 6675.0, 288.0, 1670.0, 392.0, 2630.0, 2890.0, 4150.0, 990.0], [\"2022-06-27\", 5900.0, 9500.0, 7200.0, 4980.0, 2960.0, 1660.0, 3970.0, 7350.0, 8050.0, 1500.0, 1500.0, 4280.0, 1695.0, 2650.0, 1890.0, 760.0, 5900.0, 7900.0, 4150.0, 7325.0, 1485.0, 29200.0, 6650.0, 284.0, 1665.0, 394.0, 2680.0, 2920.0, 4070.0, 1025.0], [\"2022-06-28\", 5775.0, 9500.0, 7250.0, 5050.0, 2950.0, 1685.0, 4030.0, 7300.0, 8075.0, 1500.0, 1500.0, 4300.0, 1700.0, 2650.0, 1865.0, 775.0, 5825.0, 7775.0, 4030.0, 7100.0, 1485.0, 29800.0, 6700.0, 272.0, 1665.0, 392.0, 2630.0, 2910.0, 4030.0, 1050.0], [\"2022-06-29\", 5925.0, 9450.0, 7100.0, 4940.0, 2870.0, 1670.0, 3990.0, 7275.0, 8025.0, 1475.0, 1475.0, 4180.0, 1690.0, 2680.0, 1835.0, 760.0, 5775.0, 7700.0, 3940.0, 7300.0, 1525.0, 28525.0, 6500.0, 264.0, 1665.0, 390.0, 2550.0, 2870.0, 4040.0, 1040.0], [\"2022-06-30\", 6000.0, 9550.0, 7050.0, 4770.0, 2860.0, 1590.0, 3820.0, 7250.0, 7850.0, 1455.0, 1455.0, 4150.0, 1660.0, 2750.0, 1800.0, 755.0, 5650.0, 7600.0, 3990.0, 7125.0, 1450.0, 28400.0, 6625.0, 282.0, 1605.0, 388.0, 2600.0, 2940.0, 4000.0, 1100.0], [\"2022-07-01\", 5925.0, 9350.0, 7025.0, 4830.0, 2720.0, 1600.0, 3720.0, 7250.0, 7850.0, 1420.0, 1420.0, 4140.0, 1685.0, 2700.0, 1750.0, 730.0, 5350.0, 7425.0, 3960.0, 6875.0, 1385.0, 26500.0, 6525.0, 266.0, 1560.0, 370.0, 2560.0, 2910.0, 4020.0, 1095.0], [\"2022-07-04\", 5775.0, 9225.0, 6825.0, 4700.0, 2770.0, 1550.0, 3680.0, 7050.0, 7650.0, 1400.0, 1400.0, 4040.0, 1635.0, 2700.0, 1720.0, 725.0, 5450.0, 7450.0, 3880.0, 6800.0, 1380.0, 26525.0, 6500.0, 256.0, 1550.0, 346.0, 2400.0, 2950.0, 3990.0, 1075.0], [\"2022-07-05\", 5575.0, 9175.0, 6925.0, 4760.0, 2880.0, 1610.0, 3950.0, 7250.0, 7750.0, 1400.0, 1400.0, 4090.0, 1690.0, 2660.0, 1790.0, 710.0, 5450.0, 7400.0, 4050.0, 6700.0, 1390.0, 27975.0, 6325.0, 256.0, 1585.0, 334.0, 2380.0, 2930.0, 3990.0, 1120.0], [\"2022-07-06\", 5725.0, 9450.0, 7000.0, 4780.0, 2770.0, 1530.0, 3860.0, 7300.0, 7650.0, 1400.0, 1400.0, 4080.0, 1685.0, 2700.0, 1720.0, 715.0, 5225.0, 7325.0, 3810.0, 6725.0, 1305.0, 26800.0, 6075.0, 252.0, 1540.0, 324.0, 2240.0, 2970.0, 4020.0, 1130.0], [\"2022-07-07\", 5650.0, 9475.0, 6900.0, 5000.0, 2770.0, 1535.0, 3850.0, 7100.0, 7600.0, 1395.0, 1395.0, 4050.0, 1680.0, 2660.0, 1720.0, 800.0, 5050.0, 7275.0, 3820.0, 6625.0, 1320.0, 26775.0, 5950.0, 254.0, 1575.0, 338.0, 2260.0, 2990.0, 4040.0, 1135.0], [\"2022-07-08\", 5750.0, 9425.0, 7025.0, 4890.0, 2830.0, 1560.0, 3930.0, 7150.0, 7600.0, 1395.0, 1395.0, 4200.0, 1670.0, 2700.0, 1715.0, 785.0, 5300.0, 7275.0, 3890.0, 6675.0, 1330.0, 27450.0, 5975.0, 258.0, 1650.0, 350.0, 2300.0, 2980.0, 4010.0, 1180.0], [\"2022-07-11\", 5700.0, 9450.0, 7025.0, 4800.0, 2830.0, 1585.0, 3870.0, 7125.0, 7500.0, 1390.0, 1390.0, 4120.0, 1700.0, 2800.0, 1720.0, 820.0, 5150.0, 7525.0, 3810.0, 6500.0, 1325.0, 27225.0, 6050.0, 256.0, 1655.0, 338.0, 2250.0, 2940.0, 4040.0, 1165.0], [\"2022-07-12\", 5600.0, 9325.0, 7050.0, 4880.0, 2920.0, 1595.0, 3980.0, 7175.0, 7450.0, 1430.0, 1430.0, 4110.0, 1680.0, 2750.0, 1740.0, 825.0, 5150.0, 7475.0, 3750.0, 6400.0, 1360.0, 26925.0, 6100.0, 256.0, 1675.0, 330.0, 2300.0, 2870.0, 4010.0, 1145.0], [\"2022-07-13\", 5650.0, 9300.0, 6825.0, 4900.0, 2930.0, 1540.0, 4030.0, 7000.0, 7225.0, 1410.0, 1410.0, 4060.0, 1690.0, 2750.0, 1720.0, 875.0, 5025.0, 7500.0, 3610.0, 6400.0, 1335.0, 28275.0, 5825.0, 252.0, 1605.0, 324.0, 2300.0, 2900.0, 3940.0, 1150.0], [\"2022-07-14\", 5875.0, 9250.0, 6925.0, 4900.0, 2930.0, 1570.0, 4060.0, 7025.0, 7275.0, 1390.0, 1390.0, 4070.0, 1720.0, 2790.0, 1650.0, 900.0, 5075.0, 7625.0, 3570.0, 6525.0, 1315.0, 29800.0, 6100.0, 252.0, 1625.0, 332.0, 2280.0, 2980.0, 4020.0, 1145.0], [\"2022-07-15\", 5600.0, 9350.0, 6900.0, 4830.0, 2760.0, 1530.0, 3940.0, 7000.0, 7275.0, 1395.0, 1395.0, 4110.0, 1680.0, 2750.0, 1540.0, 885.0, 4770.0, 7450.0, 3350.0, 6375.0, 1270.0, 29025.0, 5950.0, 254.0, 1700.0, 320.0, 2320.0, 3020.0, 4150.0, 1140.0], [\"2022-07-18\", 5750.0, 9500.0, 7050.0, 4980.0, 2790.0, 1565.0, 3890.0, 7150.0, 7325.0, 1405.0, 1405.0, 4140.0, 1655.0, 2750.0, 1560.0, 895.0, 4880.0, 7650.0, 3510.0, 6500.0, 1300.0, 28850.0, 6025.0, 256.0, 1745.0, 298.0, 2340.0, 2980.0, 4190.0, 1150.0], [\"2022-07-19\", 5800.0, 9450.0, 7150.0, 4970.0, 2950.0, 1585.0, 3980.0, 7175.0, 7300.0, 1435.0, 1435.0, 4110.0, 1700.0, 2750.0, 1655.0, 910.0, 5150.0, 7650.0, 3770.0, 6550.0, 1375.0, 29100.0, 6075.0, 268.0, 1830.0, 302.0, 2500.0, 2990.0, 4160.0, 1150.0], [\"2022-07-20\", 5900.0, 9450.0, 7100.0, 4970.0, 3020.0, 1605.0, 4030.0, 7400.0, 7675.0, 1465.0, 1465.0, 4300.0, 1730.0, 2830.0, 1695.0, 930.0, 5150.0, 7600.0, 3830.0, 6500.0, 1400.0, 30175.0, 6275.0, 288.0, 1855.0, 308.0, 2530.0, 3030.0, 4210.0, 1140.0], [\"2022-07-21\", 6000.0, 9500.0, 7175.0, 5000.0, 3020.0, 1580.0, 4020.0, 7400.0, 7725.0, 1470.0, 1470.0, 4310.0, 1680.0, 2760.0, 1675.0, 940.0, 5200.0, 7800.0, 3780.0, 6525.0, 1380.0, 30000.0, 6175.0, 280.0, 1865.0, 300.0, 2460.0, 3120.0, 4260.0, 1145.0], [\"2022-07-22\", 5925.0, 9400.0, 7050.0, 5000.0, 3030.0, 1605.0, 4050.0, 7325.0, 7700.0, 1465.0, 1465.0, 4270.0, 1715.0, 2700.0, 1780.0, 935.0, 5700.0, 7725.0, 4030.0, 6525.0, 1430.0, 30075.0, 6075.0, 296.0, 1960.0, 308.0, 2460.0, 3150.0, 4230.0, 1145.0], [\"2022-07-25\", 5975.0, 9450.0, 7025.0, 4930.0, 3100.0, 1600.0, 4060.0, 7300.0, 7600.0, 1460.0, 1460.0, 4250.0, 1690.0, 2700.0, 1755.0, 925.0, 5575.0, 7650.0, 3920.0, 6500.0, 1405.0, 30575.0, 6075.0, 282.0, 1860.0, 300.0, 2450.0, 3150.0, 4240.0, 1165.0], [\"2022-07-26\", 5850.0, 9375.0, 7000.0, 4930.0, 3160.0, 1590.0, 4090.0, 7300.0, 7675.0, 1455.0, 1455.0, 4330.0, 1695.0, 2680.0, 1790.0, 885.0, 5700.0, 7650.0, 3920.0, 6575.0, 1425.0, 30600.0, 6100.0, 280.0, 1800.0, 292.0, 2380.0, 3240.0, 4250.0, 1165.0], [\"2022-07-27\", 5900.0, 9275.0, 7000.0, 4720.0, 3280.0, 1620.0, 4270.0, 7325.0, 7775.0, 1460.0, 1460.0, 4320.0, 1690.0, 2700.0, 1785.0, 915.0, 5650.0, 7675.0, 3930.0, 6425.0, 1435.0, 31100.0, 6100.0, 296.0, 1800.0, 294.0, 2370.0, 3180.0, 4300.0, 1200.0], [\"2022-07-28\", 5750.0, 8975.0, 6900.0, 4510.0, 3280.0, 1685.0, 4270.0, 7350.0, 7825.0, 1465.0, 1465.0, 4360.0, 1670.0, 2600.0, 2000.0, 910.0, 6175.0, 7800.0, 4140.0, 6425.0, 1505.0, 31300.0, 6050.0, 300.0, 1865.0, 306.0, 2380.0, 3090.0, 4240.0, 1190.0], [\"2022-07-29\", 5725.0, 8875.0, 6875.0, 4510.0, 3290.0, 1680.0, 4320.0, 7425.0, 7875.0, 1485.0, 1485.0, 4380.0, 1620.0, 2520.0, 1960.0, 915.0, 6100.0, 7725.0, 4110.0, 6500.0, 1465.0, 32425.0, 6350.0, 298.0, 1855.0, 304.0, 2340.0, 3060.0, 4230.0, 1195.0]];\n",
       "\n",
       "        // Define the dt_args\n",
       "        let dt_args = {\"layout\": {\"topStart\": \"pageLength\", \"topEnd\": \"search\", \"bottomStart\": \"info\", \"bottomEnd\": \"paging\"}, \"order\": [], \"warn_on_selected_rows_not_rendered\": true};\n",
       "        dt_args[\"data\"] = data;\n",
       "\n",
       "        \n",
       "        new DataTable(table, dt_args);\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dictionary to store stock data\n",
    "stock_data = {}\n",
    "\n",
    "# Load each file into a dictionary\n",
    "for stock, path in file_paths.items():\n",
    "    try:\n",
    "        if path.endswith('.csv'):\n",
    "            df = pd.read_csv(path, index_col=\"Date\", parse_dates=True)\n",
    "        elif path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(path, index_col=\"Date\", parse_dates=True)\n",
    "        else:\n",
    "            print(f\"Unsupported file format for {stock}: {path}\")\n",
    "            continue\n",
    "        stock_data[stock] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {stock} from {path}: {e}\")\n",
    "\n",
    "# Print the first few rows of each DataFrame to debug\n",
    "for stock, df in stock_data.items():\n",
    "    print(f\"Data for {stock}:\")\n",
    "    print(df.head())\n",
    "\n",
    "# Combine all data into a single DataFrame using the \"Close\" column\n",
    "merged_df = pd.DataFrame()\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Data preview:\")\n",
    "\n",
    "for stock, df in stock_data.items():\n",
    "    if \"Close\" in df.columns:\n",
    "        merged_df[stock] = df[\"Close\"]\n",
    "    else:\n",
    "        print(f\"Warning: 'Close' column not found in {stock}\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "# Display merged stock data\n",
    "tools.display_dataframe_to_user(name=\"Merged Stock Data\", dataframe=merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "def train_multivariate_lstm(df, stock_name,\n",
    "                            seq_len=60,\n",
    "                            epochs=50,\n",
    "                            batch_size=32,\n",
    "                            units=50,\n",
    "                            dropout=0.2,\n",
    "                            learning_rate=0.001):\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    print(f\"\\n Training LSTM model for {stock_name}...\")\n",
    "\n",
    "    # 1. Prepare input features\n",
    "    features = df[['Close', 'Volume', 'Dividends', 'Stock Splits']].copy().dropna()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # 2. Split\n",
    "    split = int(len(scaled) * 0.8)\n",
    "    train_data = scaled[:split]\n",
    "    val_data = scaled[split - seq_len:]\n",
    "\n",
    "    def create_sequences(data):\n",
    "        X, y = [], []\n",
    "        for i in range(seq_len, len(data)):\n",
    "            X.append(data[i - seq_len:i])\n",
    "            y.append(data[i, 0])  # 'Close'\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_train, y_train = create_sequences(train_data)\n",
    "    X_val, y_val = create_sequences(val_data)\n",
    "\n",
    "    # 3. Build model\n",
    "    model = Sequential([\n",
    "        LSTM(units, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dropout(dropout),\n",
    "        LSTM(units),\n",
    "        Dropout(dropout),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # 4. Save model\n",
    "    model.save(f'model_{stock_name}_SL{seq_len}_U{units}_DO{dropout}_BS{batch_size}_LR{learning_rate}.h5')\n",
    "\n",
    "    # 5. Predict\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_full = np.zeros((len(y_pred), 4))\n",
    "    y_pred_full[:, 0] = y_pred[:, 0]\n",
    "    y_val_full = np.zeros((len(y_val), 4))\n",
    "    y_val_full[:, 0] = y_val\n",
    "    y_pred_real = scaler.inverse_transform(y_pred_full)[:, 0]\n",
    "    y_val_real = scaler.inverse_transform(y_val_full)[:, 0]\n",
    "\n",
    "    # 6. RMSE + Plot\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_real, y_pred_real))\n",
    "    print(f\" RMSE (validation): {rmse:.2f}\")\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(y_val_real, label='Actual Price')\n",
    "    # plt.plot(y_pred_real, label='Predicted Price')\n",
    "    # plt.title(f\"{stock_name} - Actual vs Predicted (Validation)\")\n",
    "    # plt.xlabel(\"Days\")\n",
    "    # plt.ylabel(\"Price (IDR)\")\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    return model, scaler, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def tune_lstm_hyperparameters(df, stock_name):\n",
    "    # Define ranges to test\n",
    "    seq_lens = [30, 60, 90]\n",
    "    lstm_units = [50, 100, 150]\n",
    "    dropout_rates = [0.2, 0.3]\n",
    "    batch_sizes = [16, 32]\n",
    "    learning_rates = [0.001, 0.0005]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    combinations = list(product(seq_lens, lstm_units, dropout_rates, batch_sizes, learning_rates))\n",
    "\n",
    "    print(f\" Testing {len(combinations)} combinations...\\n\")\n",
    "\n",
    "    for seq_len, units, dropout, batch, lr in combinations:\n",
    "        print(f\" Trying: seq_len={seq_len}, units={units}, dropout={dropout}, batch_size={batch}, lr={lr}\")\n",
    "        try:\n",
    "            _, _, rmse = train_multivariate_lstm(\n",
    "                df=df,\n",
    "                stock_name=stock_name,\n",
    "                seq_len=seq_len,\n",
    "                epochs=30,  \n",
    "                batch_size=batch,\n",
    "                units=units,\n",
    "                dropout=dropout,\n",
    "                learning_rate=lr\n",
    "            )\n",
    "            results.append({\n",
    "                'seq_len': seq_len,\n",
    "                'units': units,\n",
    "                'dropout': dropout,\n",
    "                'batch_size': batch,\n",
    "                'learning_rate': lr,\n",
    "                'rmse': rmse\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\" Skipped due to error: {e}\")\n",
    "\n",
    "    results.sort(key=lambda x: x['rmse'])\n",
    "\n",
    "    print(\"\\n Top 5 Results:\")\n",
    "    for r in results[:5]:\n",
    "        print(r)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing 72 combinations...\n",
      "\n",
      " Trying: seq_len=30, units=50, dropout=0.2, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      " RMSE (validation): 156.60\n",
      " Trying: seq_len=30, units=50, dropout=0.2, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      " RMSE (validation): 233.33\n",
      " Trying: seq_len=30, units=50, dropout=0.2, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      " RMSE (validation): 157.95\n",
      " Trying: seq_len=30, units=50, dropout=0.2, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      " RMSE (validation): 287.66\n",
      " Trying: seq_len=30, units=50, dropout=0.3, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      " RMSE (validation): 222.60\n",
      " Trying: seq_len=30, units=50, dropout=0.3, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      " RMSE (validation): 180.47\n",
      " Trying: seq_len=30, units=50, dropout=0.3, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      " RMSE (validation): 157.09\n",
      " Trying: seq_len=30, units=50, dropout=0.3, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      " RMSE (validation): 246.72\n",
      " Trying: seq_len=30, units=100, dropout=0.2, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      " RMSE (validation): 138.62\n",
      " Trying: seq_len=30, units=100, dropout=0.2, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      " RMSE (validation): 167.62\n",
      " Trying: seq_len=30, units=100, dropout=0.2, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      " RMSE (validation): 256.19\n",
      " Trying: seq_len=30, units=100, dropout=0.2, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      " RMSE (validation): 175.41\n",
      " Trying: seq_len=30, units=100, dropout=0.3, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      " RMSE (validation): 182.80\n",
      " Trying: seq_len=30, units=100, dropout=0.3, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      " RMSE (validation): 145.23\n",
      " Trying: seq_len=30, units=100, dropout=0.3, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      " RMSE (validation): 275.31\n",
      " Trying: seq_len=30, units=100, dropout=0.3, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      " RMSE (validation): 319.95\n",
      " Trying: seq_len=30, units=150, dropout=0.2, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      " RMSE (validation): 274.20\n",
      " Trying: seq_len=30, units=150, dropout=0.2, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      " RMSE (validation): 145.96\n",
      " Trying: seq_len=30, units=150, dropout=0.2, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      " RMSE (validation): 139.37\n",
      " Trying: seq_len=30, units=150, dropout=0.2, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      " RMSE (validation): 171.32\n",
      " Trying: seq_len=30, units=150, dropout=0.3, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      " RMSE (validation): 284.35\n",
      " Trying: seq_len=30, units=150, dropout=0.3, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      " RMSE (validation): 197.43\n",
      " Trying: seq_len=30, units=150, dropout=0.3, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step\n",
      " RMSE (validation): 214.48\n",
      " Trying: seq_len=30, units=150, dropout=0.3, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      " RMSE (validation): 180.18\n",
      " Trying: seq_len=60, units=50, dropout=0.2, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      " RMSE (validation): 141.33\n",
      " Trying: seq_len=60, units=50, dropout=0.2, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step\n",
      " RMSE (validation): 187.64\n",
      " Trying: seq_len=60, units=50, dropout=0.2, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
      " RMSE (validation): 286.55\n",
      " Trying: seq_len=60, units=50, dropout=0.2, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      " RMSE (validation): 188.45\n",
      " Trying: seq_len=60, units=50, dropout=0.3, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
      " RMSE (validation): 186.48\n",
      " Trying: seq_len=60, units=50, dropout=0.3, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
      " RMSE (validation): 364.08\n",
      " Trying: seq_len=60, units=50, dropout=0.3, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      " RMSE (validation): 381.27\n",
      " Trying: seq_len=60, units=50, dropout=0.3, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
      " RMSE (validation): 260.87\n",
      " Trying: seq_len=60, units=100, dropout=0.2, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      " RMSE (validation): 134.01\n",
      " Trying: seq_len=60, units=100, dropout=0.2, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      " RMSE (validation): 275.63\n",
      " Trying: seq_len=60, units=100, dropout=0.2, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
      " RMSE (validation): 159.95\n",
      " Trying: seq_len=60, units=100, dropout=0.2, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      " RMSE (validation): 165.47\n",
      " Trying: seq_len=60, units=100, dropout=0.3, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      " RMSE (validation): 154.80\n",
      " Trying: seq_len=60, units=100, dropout=0.3, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      " RMSE (validation): 166.21\n",
      " Trying: seq_len=60, units=100, dropout=0.3, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      " RMSE (validation): 308.94\n",
      " Trying: seq_len=60, units=100, dropout=0.3, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step\n",
      " RMSE (validation): 209.05\n",
      " Trying: seq_len=60, units=150, dropout=0.2, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step\n",
      " RMSE (validation): 182.61\n",
      " Trying: seq_len=60, units=150, dropout=0.2, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step\n",
      " RMSE (validation): 139.89\n",
      " Trying: seq_len=60, units=150, dropout=0.2, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step\n",
      " RMSE (validation): 148.91\n",
      " Trying: seq_len=60, units=150, dropout=0.2, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step\n",
      " RMSE (validation): 175.99\n",
      " Trying: seq_len=60, units=150, dropout=0.3, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step\n",
      " RMSE (validation): 157.38\n",
      " Trying: seq_len=60, units=150, dropout=0.3, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step\n",
      " RMSE (validation): 204.47\n",
      " Trying: seq_len=60, units=150, dropout=0.3, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step\n",
      " RMSE (validation): 190.80\n",
      " Trying: seq_len=60, units=150, dropout=0.3, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step\n",
      " RMSE (validation): 200.56\n",
      " Trying: seq_len=90, units=50, dropout=0.2, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step\n",
      " RMSE (validation): 152.06\n",
      " Trying: seq_len=90, units=50, dropout=0.2, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step\n",
      " RMSE (validation): 274.80\n",
      " Trying: seq_len=90, units=50, dropout=0.2, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      " RMSE (validation): 216.02\n",
      " Trying: seq_len=90, units=50, dropout=0.2, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step\n",
      " RMSE (validation): 256.96\n",
      " Trying: seq_len=90, units=50, dropout=0.3, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 274ms/step\n",
      " RMSE (validation): 220.94\n",
      " Trying: seq_len=90, units=50, dropout=0.3, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step\n",
      " RMSE (validation): 371.72\n",
      " Trying: seq_len=90, units=50, dropout=0.3, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step\n",
      " RMSE (validation): 290.06\n",
      " Trying: seq_len=90, units=50, dropout=0.3, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step\n",
      " RMSE (validation): 313.84\n",
      " Trying: seq_len=90, units=100, dropout=0.2, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step\n",
      " RMSE (validation): 173.32\n",
      " Trying: seq_len=90, units=100, dropout=0.2, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step\n",
      " RMSE (validation): 260.00\n",
      " Trying: seq_len=90, units=100, dropout=0.2, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step\n",
      " RMSE (validation): 245.48\n",
      " Trying: seq_len=90, units=100, dropout=0.2, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step\n",
      " RMSE (validation): 196.46\n",
      " Trying: seq_len=90, units=100, dropout=0.3, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step\n",
      " RMSE (validation): 236.77\n",
      " Trying: seq_len=90, units=100, dropout=0.3, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step\n",
      " RMSE (validation): 268.84\n",
      " Trying: seq_len=90, units=100, dropout=0.3, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step\n",
      " RMSE (validation): 157.13\n",
      " Trying: seq_len=90, units=100, dropout=0.3, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step\n",
      " RMSE (validation): 191.64\n",
      " Trying: seq_len=90, units=150, dropout=0.2, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step\n",
      " RMSE (validation): 135.83\n",
      " Trying: seq_len=90, units=150, dropout=0.2, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step\n",
      " RMSE (validation): 140.63\n",
      " Trying: seq_len=90, units=150, dropout=0.2, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n",
      " RMSE (validation): 152.22\n",
      " Trying: seq_len=90, units=150, dropout=0.2, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      " RMSE (validation): 149.10\n",
      " Trying: seq_len=90, units=150, dropout=0.3, batch_size=16, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step\n",
      " RMSE (validation): 125.39\n",
      " Trying: seq_len=90, units=150, dropout=0.3, batch_size=16, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step\n",
      " RMSE (validation): 305.89\n",
      " Trying: seq_len=90, units=150, dropout=0.3, batch_size=32, lr=0.001\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
      " RMSE (validation): 437.96\n",
      " Trying: seq_len=90, units=150, dropout=0.3, batch_size=32, lr=0.0005\n",
      "\n",
      " Training LSTM model for BBCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wesley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      " RMSE (validation): 161.74\n",
      "\n",
      " Top 5 Results:\n",
      "{'seq_len': 90, 'units': 150, 'dropout': 0.3, 'batch_size': 16, 'learning_rate': 0.001, 'rmse': np.float64(125.38762397829232)}\n",
      "{'seq_len': 60, 'units': 100, 'dropout': 0.2, 'batch_size': 16, 'learning_rate': 0.001, 'rmse': np.float64(134.0058746140335)}\n",
      "{'seq_len': 90, 'units': 150, 'dropout': 0.2, 'batch_size': 16, 'learning_rate': 0.001, 'rmse': np.float64(135.8263024698593)}\n",
      "{'seq_len': 30, 'units': 100, 'dropout': 0.2, 'batch_size': 16, 'learning_rate': 0.001, 'rmse': np.float64(138.6232193728952)}\n",
      "{'seq_len': 30, 'units': 150, 'dropout': 0.2, 'batch_size': 32, 'learning_rate': 0.001, 'rmse': np.float64(139.37445386226418)}\n"
     ]
    }
   ],
   "source": [
    "BBCA_df = stock_data['BBCA']\n",
    "results = tune_lstm_hyperparameters(BBCA_df, \"BBCA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
